{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Coding Challenge Week 3**\n",
    "**DE: Web Scraping Pipeline**\n",
    "\n",
    "**Author: Lalit Gupta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jovian\n",
    "# !pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_url = 'https://books.toscrape.com/catalogue/page-'\n",
    "# webpage = requests.get(web_url)\n",
    "# content = webpage.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save content as html file\n",
    "# with open('webpage.html', 'w', encoding='utf-8') as f:\n",
    "#     f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WebScrapper Class** \n",
    "\n",
    "Handles Everything from Scraping, Parsing to Preprocessing of webpage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebBookScraper():\n",
    "\n",
    "    def __init__(self, url, limit=1):\n",
    "        \"\"\"\n",
    "        Initializes the object with a URL and an optional limit.\n",
    "\n",
    "        Parameters:\n",
    "        - url: The URL to fetch data from.\n",
    "        - limit: The limit on the number of items to fetch (default is 1).\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.limit = limit\n",
    "\n",
    "        # Create an empty DataFrame to store data\n",
    "        self.df = pd.DataFrame(columns=['Title', 'Price', 'Availability','Rating'])\n",
    "        self.df.to_csv('books.csv', index=False)\n",
    "    \n",
    "    def initiate(self):\n",
    "        self.extract_info(self.url)\n",
    "\n",
    "    def log(self, message):\n",
    "        '''Prints a message to the console'''\n",
    "        print(\"DEBUG: \" + str(message))\n",
    "\n",
    "    def transform_titles(self, titles):\n",
    "        \"\"\"\n",
    "        Remove extra whitespace from book titles\n",
    "        Remove commas from book titles\n",
    "        Remove . from book titles\n",
    "        \"\"\"\n",
    "        titles = [title.replace('\\n','').replace(',','').replace('.','') for title in titles]\n",
    "        return titles\n",
    "\n",
    "    def get_book_titles(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts book titles from the given HTML document.\n",
    "\n",
    "        Parameters:\n",
    "        - doc: BeautifulSoup object representing the HTML document.\n",
    "\n",
    "        Returns:\n",
    "        - titles: List of book titles.\n",
    "        \"\"\"\n",
    "        # title_tags = doc.find_all('h3 a[title]')\n",
    "        books = doc.find_all(\"article\", class_=\"product_pod\")\n",
    "        titles = [book.find(\"h3\").find(\"a\").get(\"title\") for book in books]\n",
    "        # titles = [tag.text for tag in title_tags]\n",
    "        #remove the extra whitespace\n",
    "        titles = self.transform_titles(titles)\n",
    "\n",
    "        return titles\n",
    "\n",
    "    def get_book_prices(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts book prices from the given HTML document.\n",
    "\n",
    "        Parameters:\n",
    "        - doc: BeautifulSoup object representing the HTML document.\n",
    "\n",
    "        Returns:\n",
    "        - prices: List of book prices.\n",
    "        \"\"\"\n",
    "        price_tags = doc.find_all('p', class_='price_color')\n",
    "        prices = [tag.text.replace('Ã‚', '') for tag in price_tags]\n",
    "        #convert prices to numeric values and remove the pound sign\n",
    "        prices = [float(price[1:]) for price in prices]\n",
    "        return prices\n",
    "    \n",
    "    def convert_rating(self, rating):\n",
    "        \"\"\"\n",
    "        Converts rating from string to integer.\n",
    "\n",
    "        Parameters:\n",
    "        - rating: The string value for the rating.\n",
    "\n",
    "        Returns:\n",
    "        - rating: The integer value for the rating.\n",
    "        \"\"\"\n",
    "        RATINGS = {\n",
    "            'One': 1,\n",
    "            'Two': 2,\n",
    "            'Three': 3,\n",
    "            'Four': 4,\n",
    "            'Five': 5\n",
    "        }\n",
    "        return RATINGS[rating]\n",
    "\n",
    "    def get_book_ratings(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts book ratings from the given HTML document.\n",
    "\n",
    "        Parameters:\n",
    "        - doc: BeautifulSoup object representing the HTML document.\n",
    "\n",
    "        Returns:\n",
    "        - ratings: List of book ratings.\n",
    "        \"\"\"\n",
    "        rating_tags = doc.find_all('p', class_='star-rating')\n",
    "        ratings = [tag.get('class')[1] for tag in rating_tags]\n",
    "        #convert ratings to numeric values\n",
    "        ratings = [self.convert_rating(rating) for rating in ratings]\n",
    "        return ratings\n",
    "\n",
    "    def get_stock_availability(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts stock availability information from the given HTML document.\n",
    "\n",
    "        Parameters:\n",
    "        - doc: BeautifulSoup object representing the HTML document.\n",
    "\n",
    "        Returns:\n",
    "        - availability: List of stock availability information.\n",
    "        \"\"\"\n",
    "        stock_tags = doc.find_all('p', class_='instock availability')\n",
    "        availability = [tag.text.strip() for tag in stock_tags]\n",
    "        #If IN STOCK, then True, else False\n",
    "        availability = [True if stock == 'In stock' else False for stock in availability]\n",
    "        return availability\n",
    "\n",
    "    def get_webpage(url):\n",
    "        '''Get the webpage content'''\n",
    "        webpage = requests.get(url)\n",
    "        content = webpage.text\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        if webpage.status_code != 200:\n",
    "            raise Exception('Failed to load page {}'.format(url))\n",
    "        return soup\n",
    "    \n",
    "    def get_book_info(self, soup):\n",
    "        '''Get the book info from the webpage'''\n",
    "        titles,prices,availability,ratings = [],[],[],[]\n",
    "        titles = self.get_book_titles(soup)\n",
    "        prices = self.get_book_prices(soup)\n",
    "        availability = self.get_stock_availability(soup)\n",
    "        ratings = self.get_book_ratings(soup)\n",
    "        return titles,prices,availability,ratings\n",
    "\n",
    "    def extract_info(self, url):\n",
    "        '''Pipeline for extracting data from the webpage'''\n",
    "        for page in range(1,self.limit+1):\n",
    "            self.log('Extracting page {} of {}'.format(page,self.limit))\n",
    "            URL = url + str(page) + '.html'\n",
    "            soup = WebBookScraper.get_webpage(URL)\n",
    "            titles,prices,availability,ratings = self.get_book_info(soup)\n",
    "            # create a dataframe from the extracted data\n",
    "            df = pd.DataFrame({'Title':titles, 'Price':prices, 'Availability':availability, 'Rating':ratings})\n",
    "            # append the dataframe to a csv file\n",
    "            self.log('Saving books...')\n",
    "            df.to_csv('books.csv', mode='a', header=False, index=False)\n",
    "            self.log(\"Saved {} books from page {} of {}\".format(len(titles),page,self.limit))\n",
    "        self.log('Extraction complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating Web Scrapping ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Extracting page 1 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 1 of 50\n",
      "DEBUG: Extracting page 2 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 2 of 50\n",
      "DEBUG: Extracting page 3 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 3 of 50\n",
      "DEBUG: Extracting page 4 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 4 of 50\n",
      "DEBUG: Extracting page 5 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 5 of 50\n",
      "DEBUG: Extracting page 6 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 6 of 50\n",
      "DEBUG: Extracting page 7 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 7 of 50\n",
      "DEBUG: Extracting page 8 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 8 of 50\n",
      "DEBUG: Extracting page 9 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 9 of 50\n",
      "DEBUG: Extracting page 10 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 10 of 50\n",
      "DEBUG: Extracting page 11 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 11 of 50\n",
      "DEBUG: Extracting page 12 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 12 of 50\n",
      "DEBUG: Extracting page 13 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 13 of 50\n",
      "DEBUG: Extracting page 14 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 14 of 50\n",
      "DEBUG: Extracting page 15 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 15 of 50\n",
      "DEBUG: Extracting page 16 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 16 of 50\n",
      "DEBUG: Extracting page 17 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 17 of 50\n",
      "DEBUG: Extracting page 18 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 18 of 50\n",
      "DEBUG: Extracting page 19 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 19 of 50\n",
      "DEBUG: Extracting page 20 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 20 of 50\n",
      "DEBUG: Extracting page 21 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 21 of 50\n",
      "DEBUG: Extracting page 22 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 22 of 50\n",
      "DEBUG: Extracting page 23 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 23 of 50\n",
      "DEBUG: Extracting page 24 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 24 of 50\n",
      "DEBUG: Extracting page 25 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 25 of 50\n",
      "DEBUG: Extracting page 26 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 26 of 50\n",
      "DEBUG: Extracting page 27 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 27 of 50\n",
      "DEBUG: Extracting page 28 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 28 of 50\n",
      "DEBUG: Extracting page 29 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 29 of 50\n",
      "DEBUG: Extracting page 30 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 30 of 50\n",
      "DEBUG: Extracting page 31 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 31 of 50\n",
      "DEBUG: Extracting page 32 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 32 of 50\n",
      "DEBUG: Extracting page 33 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 33 of 50\n",
      "DEBUG: Extracting page 34 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 34 of 50\n",
      "DEBUG: Extracting page 35 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 35 of 50\n",
      "DEBUG: Extracting page 36 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 36 of 50\n",
      "DEBUG: Extracting page 37 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 37 of 50\n",
      "DEBUG: Extracting page 38 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 38 of 50\n",
      "DEBUG: Extracting page 39 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 39 of 50\n",
      "DEBUG: Extracting page 40 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 40 of 50\n",
      "DEBUG: Extracting page 41 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 41 of 50\n",
      "DEBUG: Extracting page 42 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 42 of 50\n",
      "DEBUG: Extracting page 43 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 43 of 50\n",
      "DEBUG: Extracting page 44 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 44 of 50\n",
      "DEBUG: Extracting page 45 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 45 of 50\n",
      "DEBUG: Extracting page 46 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 46 of 50\n",
      "DEBUG: Extracting page 47 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 47 of 50\n",
      "DEBUG: Extracting page 48 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 48 of 50\n",
      "DEBUG: Extracting page 49 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 49 of 50\n",
      "DEBUG: Extracting page 50 of 50\n",
      "DEBUG: Saving books...\n",
      "DEBUG: Saved 20 books from page 50 of 50\n",
      "DEBUG: Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "web_url = 'https://books.toscrape.com/catalogue/page-'\n",
    "scraper = WebBookScraper(web_url,50)\n",
    "scraper.initiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Connecting to Snowflake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import os\n",
    "USER='LALITBLEND360'\n",
    "ACCOUNT='uypccvv-lnb97707'\n",
    "PASSWORD = os.getenv('SNOWSQL_PWD')\n",
    "conn = snowflake.connector.connect(\n",
    "    user=USER,\n",
    "    password=PASSWORD,\n",
    "    account=ACCOUNT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Warehouse & DataBase Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x18e0f97af50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the warehouse and database\n",
    "conn.cursor().execute(\"CREATE WAREHOUSE IF NOT EXISTS dw_blend360\")\n",
    "conn.cursor().execute(\"USE WAREHOUSE dw_blend360\")\n",
    "\n",
    "conn.cursor().execute(\"CREATE DATABASE IF NOT EXISTS books\")\n",
    "conn.cursor().execute(\"USE DATABASE books\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema and Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x18e7b1571d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.cursor().execute(\"CREATE SCHEMA IF NOT EXISTS public\")\n",
    "conn.cursor().execute(\"USE SCHEMA public\")\n",
    "# also create a table in the database\n",
    "\n",
    "table_query = \"\"\"\n",
    "CREATE OR REPLACE TABLE table_books (\n",
    "    Title VARCHAR,\n",
    "    Price FLOAT,\n",
    "    Availability BOOLEAN,\n",
    "    Rating INTEGER\n",
    ")\n",
    "\"\"\"\n",
    "conn.cursor().execute(table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inserting data file to Snowflake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x18e0f161650>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a stage\n",
    "conn.cursor().execute(\"CREATE OR REPLACE STAGE books_stage\")\n",
    "# loading the data from the csv file to the stage\n",
    "conn.cursor().execute(\"PUT file://books.csv @books_stage auto_compress=true\")\n",
    "# copying the data from the stage to the table\n",
    "conn.cursor().execute(\"COPY INTO table_books FROM @books_stage/  FILE_FORMAT=(TYPE=CSV FIELD_DELIMITER=',' SKIP_HEADER=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data from snowflake table\n",
    "# conn.cursor().execute(\"SELECT * FROM table_books\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
